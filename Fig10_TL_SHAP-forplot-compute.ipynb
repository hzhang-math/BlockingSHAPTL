{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to \n",
    "- compute the SHAP value change before and after transfer learning \n",
    "- plot the SHAP value maps\n",
    "- notice that the prob in the title is that f(x) becomes committor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/hz1994/blocking/data_MMmodel/dim/\n",
      "/scratch/hz1994/blocking/data_MMmodel/nondim/\n",
      "/scratch/hz1994/blocking/MMmodel/MMmodel/code_Lucarini/\n",
      "/scratch/hz1994/blocking/data_MMmodel/DGindex/\n",
      "/scratch/hz1994/blocking/data_MMmodel/conditionT/\n",
      "/scratch/hz1994/blocking/data_MMmodel/CNNmodels/\n",
      "/scratch/hz1994/blocking/data_MMmodel/fig_MMmodel/\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cartopy\n",
    "from cartopy import crs as ccrs\n",
    "import matplotlib \n",
    "matplotlib.rcParams[\"font.size\"] = 12\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join, exists\n",
    "from os import mkdir\n",
    "import scipy\n",
    "import netCDF4\n",
    "import sklearn\n",
    "import sys\n",
    "import matplotlib.ticker as mticker\n",
    "from cartopy.mpl.ticker import (LongitudeFormatter, LatitudeFormatter,\n",
    "                                LatitudeLocator, LongitudeLocator)\n",
    "%matplotlib inline\n",
    "import matplotlib.path as mpath\n",
    "import importlib.util\n",
    "import MM_util_AI\n",
    "import MM_utilplot\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_score\n",
    "import matplotlib.colors as colors\n",
    "import os\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"MM_dataprepare\", \\\n",
    "                        \"/scratch/hz1994/blocking/MMmodel/MMmodel/notebooks/MM_dataprepare.py\")\n",
    "MM_dataprepare = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"module.name\"] = MM_dataprepare\n",
    "spec.loader.exec_module(MM_dataprepare)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"MM_utilblocking\", \\\n",
    "                        \"/scratch/hz1994/blocking/MMmodel/MMmodel/notebooks/MM_utilblocking.py\")\n",
    "MM_utilblocking = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"module.name\"] = MM_utilblocking\n",
    "spec.loader.exec_module(MM_utilblocking)\n",
    "\n",
    " \n",
    "            \n",
    "with open(\"/scratch/hz1994/blocking/data_MMmodel/filepath.txt\",\"r\") as fi:\n",
    "    for ln in fi:\n",
    "        if ln.startswith(\"dimensionalized_filepath\"):\n",
    "            dim_path=ln.strip().split('\\t')[1]\n",
    "        if ln.startswith(\"nondimensionalized_filepath\"):\n",
    "            nondim_path=ln.strip().split('\\t')[1]\n",
    "        if ln.startswith(\"code_filepath\"):\n",
    "            code_path=ln.strip().split('\\t')[1]            \n",
    "        if ln.startswith(\"DGindex_filepath\"):\n",
    "            DGindex_path=ln.strip().split('\\t')[1]  \n",
    "        if ln.startswith(\"conditionedT_filepath\" ):\n",
    "            train_path=ln.strip().split('\\t')[1]\n",
    "        if ln.startswith(\"model_filepath\" ):\n",
    "            models_path=ln.strip().split('\\t')[1]\n",
    "        if ln.startswith(\"fig_filepath\" ):\n",
    "            fig_path=ln.strip().split('\\t')[1] \n",
    "print(dim_path)\n",
    "print(nondim_path)\n",
    "print(code_path)\n",
    "print(DGindex_path)\n",
    "print(train_path)\n",
    "print(models_path)\n",
    "print(fig_path)\n",
    "train_path_setA=train_path+'T/'\n",
    "models_path_setA=models_path+'T/'\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'regular',\n",
    "        'size'   : 16}\n",
    "plt.rc('font', **font)\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_score\n",
    "def test_model(model,weightpath,test_data,test_classlabels,test_labels):\n",
    "    model.load_weights(weightpath)\n",
    "    predictions = model.predict(test_data)\n",
    "    pred=(predictions[:,0]<predictions[:,1])\n",
    "    bce=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    loss=bce(test_classlabels, predictions).numpy()\n",
    "    TN,FP,FN,TP = confusion_matrix(test_labels, pred).flatten()\n",
    "    recall=recall_score(test_labels, pred , labels=1 )\n",
    "    precision=precision_score(test_labels, pred , labels=1 )\n",
    "    return np.array([TN,FP,FN,TP]),recall,precision,loss\n",
    "\n",
    "def polorplot(ax,data_xr,max_abs,levels,iv=0.02):\n",
    "    norm = colors.TwoSlopeNorm(vmin=-max_abs, vcenter=0, vmax=max_abs)\n",
    "    im=xr.plot.contourf( \n",
    "        data_xr,\n",
    "        x=\"longitude\", y=\"latitude\", ax=ax,transform=ccrs.PlateCarree(),cmap='coolwarm',\\\n",
    "        levels=levels, add_colorbar=False,norm=norm\n",
    "    )\n",
    "    gl=ax.gridlines(draw_labels=False)\n",
    "    gl.ylocator = mticker.FixedLocator([20,50,60,70])  \n",
    "    ax.coastlines()\n",
    "    \n",
    "    return ax,im\n",
    "\n",
    "def polorplot_levels(plotmap,latitudes,longitudes ,minval,maxval,label=\"SHAP values\", number_levels=30,iv=0.02):\n",
    "    fig,ax = plt.subplots(figsize=(9,3), \n",
    "            subplot_kw={'projection': ccrs.NorthPolarStereo()},ncols=3)\n",
    "    titles=[\"Z200\",\"Z500\",\"Z800\"]\n",
    "    max_abs=max(abs(minval),abs(maxval))\n",
    "    print(\"min plotmap=\", plotmap.min() ,\"max plotmap=\", plotmap.max() ,)\n",
    "    for i in range(3):\n",
    "        y=plotmap[:,:,i]\n",
    "        a = xr.DataArray(y, \n",
    "            coords={'latitude':latitudes,'longitude': longitudes,}, \n",
    "            dims=[\"latitude\",\"longitude\",])\n",
    "        ax[i],im =polorplot( ax[i],a, max_abs=max_abs,\\\n",
    "                            levels=np.linspace(minval,maxval,10 ), iv=iv)\n",
    "        ax[i].set_title(titles[i])\n",
    "        \n",
    "    cbar_ax = fig.add_axes([0.05, -0.1, .9, .05]) #left, bottom, width, height\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation=\"horizontal\",cmap='coolwarm',\\\n",
    "                        ticks= np.arange(iv*int(minval/iv),iv*int(maxval/iv)+iv,iv),\\\n",
    "                        label = label, shrink = 1,)\n",
    "    print( np.linspace(minval,maxval,10 ))\n",
    "#     plt.subplots_adjust(wspace=0.1,width_ratios=[1,1,1])\n",
    "    fig.tight_layout()\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Duration=5\n",
    "cnnsize=\"normal\"\n",
    "data_amount=\"1000.0k\"\n",
    "random_seed=30\n",
    "epsilon=0\n",
    "trainable_layer_number=7\n",
    "learning_rate=0.0001  \n",
    "epoch=2\n",
    "epoch_TL=4  \n",
    "\n",
    "\n",
    "X=np.load(\"/scratch/hz1994/blocking/data_era5/\"+\"test_data_1940-2022.npy\")  \n",
    "Ysparse=np.load(\"/scratch/hz1994/blocking/data_era5/\"+\"test_labels_1940-2022_T%d.npy\"%Duration)\n",
    "Y=np.zeros((Ysparse.size,2)).astype(bool)\n",
    "Y[:,1][Ysparse==1]=True  #blocking\n",
    "Y[:,0][Ysparse==0]=True\n",
    "latitudes = np.load(dim_path+'dataX_lat.npy')\n",
    "longitudes = np.load(dim_path+'dataX_lon.npy')\n",
    "X_dim=X[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model before transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(False)\n",
    "if cnnsize==\"smaller\":\n",
    "    base_model = MM_util_AI.make_s_model((18,90,3))\n",
    "    TL_model = MM_util_AI.make_s_model((18,90,3))\n",
    "elif cnnsize==\"smaller_smaller\":\n",
    "    base_model = MM_util_AI.make_ss_model((18,90,3))\n",
    "    TL_model = MM_util_AI.make_ss_model((18,90,3))\n",
    "elif cnnsize==\"normal\":\n",
    "    base_model = MM_util_AI.make_model((18,90,3))\n",
    "    TL_model = MM_util_AI.make_model((18,90,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick the data we want to make the plot: TP  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP_consistent=np.ones(Y.shape[0]).astype(bool)\n",
    "for random_seed in range(30,40):\n",
    "    path=\"/scratch/hz1994/blocking/data_MMmodel/CNNmodels/T/era5_retrainCNN/extreme_%ddaysblocking/trained_layer_%s/learning_rate_%.4f/\"\\\n",
    "            %( Duration,trainable_layer_number,learning_rate)+    \"data_\"+data_amount+\"_\"+cnnsize\\\n",
    "            +\"_cnn_\"+\"regularize\"+\"_%.3e\"%epsilon+\"_rs_%d\"%random_seed+\"epoch_%d/\"%epoch\n",
    "    \n",
    "    for num in range(10): \n",
    "        weightpath=path+\"%d/\"%num +\"cp-%04d.ckpt\"%epoch_TL \n",
    "        TL_model.load_weights(weightpath)\n",
    "        predictions = TL_model.predict(X,verbose=None)\n",
    "        pred_TL=(predictions[:,0]<predictions[:,1])\n",
    "        TP_consistent=np.logical_and(TP_consistent,np.logical_and(pred_TL, Ysparse))\n",
    "\n",
    "        weightpath=path+\"%d/\"%num +\"cp-%04d.ckpt\"%0 \n",
    "        TL_model.load_weights(weightpath)\n",
    "        predictions = TL_model.predict(X,verbose=None)\n",
    "        pred_TL=(predictions[:,0]<predictions[:,1])\n",
    "        TP_consistent=np.logical_and(TP_consistent,np.logical_and(pred_TL, Ysparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot1=[]\n",
    "plot2=[]\n",
    "for random_seed in range(30,40):\n",
    "    path=\"/scratch/hz1994/blocking/data_MMmodel/CNNmodels/T/era5_retrainCNN/extreme_%ddaysblocking/trained_layer_%s/learning_rate_%.4f/\"\\\n",
    "            %( Duration,trainable_layer_number,learning_rate)+    \"data_\"+data_amount+\"_\"+cnnsize\\\n",
    "            +\"_cnn_\"+\"regularize\"+\"_%.3e\"%epsilon+\"_rs_%d\"%random_seed+\"epoch_%d/\"%epoch\n",
    "    \n",
    "    for num in range(10): \n",
    "        init_shap=np.load(path+\"%d/\"%num+\"shapvalue-%04d_prob.npy\"%0)[TP_consistent,1,0]\n",
    "        TL_shap=np.load(path+\"%d/\"%num+\"shapvalue-%04d_prob.npy\"%epoch_TL )[TP_consistent,1,0]\n",
    "        plot1.append(init_shap)\n",
    "        plot2.append(TL_shap)   \n",
    "plot1_mean=plot1.mean(axis=(0,1))\n",
    "fig1,ax=polorplot_levels(plot1,latitudes,longitudes   ,minval=-0.001,maxval=0.003,iv=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the non-overfitting ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "for i in range(100):\n",
    "    a.append(np.array(plot2)[i].mean(axis=0).max())\n",
    "    b.append(np.array(plot2)[i].mean(axis=0).min())\n",
    "a=np.array(a)\n",
    "b=np.array(b)\n",
    "non_overfit=np.logical_and((abs(a)<0.003) , (abs(b)<0.0016)) # 0.003 and 0.0016 are 80% percentile of absolute value of max and mins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1_mean=np.array(plot1)[non_overfit].mean(axis=(0,1))\n",
    "plot1_mean_normalize=plot1_mean/np.sum(plot1_mean)\n",
    "plot2_mean=np.array(plot2)[non_overfit].mean(axis=(0,1))\n",
    "plot2_mean_normalize=plot2_mean/np.sum(plot2_mean) \n",
    "plot3=(np.maximum(plot2_mean_normalize,0)-np.maximum(plot1_mean_normalize,0) )*X_dim\n",
    "plot3_2=( plot2_mean_normalize - plot1_mean_normalize  )*X_dim\n",
    "np.save(\"shap_before_Fine_tuning\",plot1_mean)\n",
    "np.save(\"shap_after_Fine_tuning\",plot2_mean)\n",
    "np.save(\"normalized_shap_difference_max\",plot3)\n",
    "np.save(\"normalized_shap_difference\",plot3_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
